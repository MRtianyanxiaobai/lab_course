{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数与优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一个实验中，我们初步完成了梯度下降算法求解线性回归问题的实例。在这个过程中，我们自己定义了损失函数和权重的更新，其实 PyTorch 也为我们直接定义了相应的工具包，使我们能够简洁快速的实现损失函数、权重的更新和梯度的求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 损失函数的定义\n",
    "- 优化器的定义\n",
    "- 模型的训练步骤"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 损失函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上一个实验中，我们利用了自己定义的损失函数对线性问题进行了求解。其实 `torch.nn` 中存在很多封装好的损失函数。比如交叉熵损失，用 `torch.nn.MSELoss()` 表示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，还是让我们先把所需要的变量和数据集定义出来："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# 初始化数据集\n",
    "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
    "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
    "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)\n",
    "# 正向传播函数\n",
    "\n",
    "\n",
    "def forward(x):\n",
    "    return w * x\n",
    "\n",
    "\n",
    "# 测试代码\n",
    "pre = forward(X)\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们通过 `nn.MSELoss()` 计算此时预测值和真实值之间的损失："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 这里使用交叉熵损失计算预测值和真实值之间的距离\n",
    "loss = nn.MSELoss()\n",
    "# 测试此时的损失\n",
    "loss(forward(X), Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 优化器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "优化器可以理解为一种利用梯度下降算法自动求解所需参数的工具包。在 PyTorch 中提供了 `torch.optim` 方法优化我们的模型。 `torch.optim` 工具包中存在着各种梯度下降的改进算法，比如 SGD、Momentum、RMSProp 和 Adam 等。这些算法都是以传统梯度下降算法为基础，提出的改进算法，这些算法可以更快更准确地求解最佳模型参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以通过下面方式定义一个 SGD 优化器："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    optimizer = torch.optim.SGD([w], lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中第一个参数，表示的是损失函数中的权重，即我们需要求取的值。lr 表示的是梯度下降的步长。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于一般的模型都是复杂的多元函数，每次使用梯度下降算法时，我们都需要手动的对每个变量进行更新，这无疑是非常繁琐的。而使用优化器，我们可以一次性对所有的变量进行更新。函数如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ` optimizer.step()`  ：对损失中的相关变量进行更新，即所有参数值向梯度相反方向走一步。\n",
    "-  `optimizer.zero_grad()` ：对损失函数的相关变量进行梯度的清空。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，让我们完整的进行一次线性回归的求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，定义损失函数和优化器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义损失和优化器\n",
    "learning_rate = 0.01\n",
    "n_iters = 100\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，根据正向传播结果，更新梯度，进而更新权重值："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型的训练过程\n",
    "for epoch in range(n_iters):\n",
    "    y_predicted = forward(X)\n",
    "    #计算损失\n",
    "    l = loss(Y, y_predicted)\n",
    "    #计算梯度\n",
    "    l.backward()\n",
    "    # 更新权重，即向梯度方向走一步\n",
    "    optimizer.step()\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        print('epoch ', epoch+1, ': w = ', w, ' loss = ', l)\n",
    "\n",
    "print(f'根据训练模型预测，当 x =5 时，y 的值为： {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里我们进行了 100 次的迭代，可以发现得到的权重 w 和实际值相同，损失无限接近于 0 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 模型的建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了梯度的求解、权重的更新和梯度的清空外，PyTorch 实际上还为我们提供了模型的定义。也就是说，我们不用手动定义 `forward` 函数了。PyTorch 中为我们提供了预定义模型，可以直接使用。如 `torch.nn.Linear(input_size, output_size)` 表示线性函数模型。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- input_size：输入数据的维度\n",
    "- output_size：输出数据的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总结一下，我们可以将一个线性问题的求解分为下面三个步骤："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 定义模型（即正向传播函数）。\n",
    "2. 定义损失和优化器。\n",
    "3. 模型的训练（正向传播、反向传播、更新梯度、梯度下降、循环）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，让我们利用 PyTorch 定义线性函数模型：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 由于使用 PyTorch ，因此所有的变量都为张量\n",
    "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
    "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)\n",
    "X_test = torch.tensor([5], dtype=torch.float32)\n",
    "\n",
    "# 1. 定义模型\n",
    "n_samples, n_features = X.shape\n",
    "# 这里输入和输出的维度相同\n",
    "model = nn.Linear(n_features, n_features)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 在模型训练时，我们可以直接利用 `model(x)` 作为模型的正向传播，该函数返回数据 x 的预测结果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们定义优化器和损失函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. 定义优化器和损失函数\n",
    "learning_rate = 0.1\n",
    "n_iters = 100\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "# 在定义优化器时，直接利用 model.parameters() 表示模型中所有需要求的权重\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后，我们就可以利用上面定义的模型、优化器和损失函数进行模型的训练了（即利用梯度下降算法，求解损失最小时的权重值）："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 3. 模型的训练，固定的步骤：正向传播、计算损失、反向传播、更新权重、梯度清空\n",
    "for epoch in range(n_iters):\n",
    "    # 正向传播\n",
    "    y_predicted = model(X)\n",
    "    # 损失\n",
    "    l = loss(Y, y_predicted)\n",
    "    # 反向传播\n",
    "    l.backward()\n",
    "    # 更新权重\n",
    "    optimizer.step()\n",
    "    # 清空梯度\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if epoch % 10 == 0:\n",
    "        [w, b] = model.parameters() # unpack parameters\n",
    "        print('epoch ', epoch+1, ': w = ', w[0][0].item(), ' loss = ', l)\n",
    "\n",
    "print(f'根据训练模型预测，当 x =5 时，y 的值为：', forward(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以看到其实模型训练的步骤是固定的:\n",
    "\n",
    "1. 利用 `nn.Linear` 定义模型。\n",
    "2. 利用 `nn.MSELoss` 定义损失。\n",
    "3. 利用 `torch.optim` 定义优化器。\n",
    "4. 利用梯度下降算法进行模型的训练。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "并且模型的训练步骤也是固定的：\n",
    "\n",
    "1. 利用 `model(X)` 进行正向传播。\n",
    "2. 利用 `loss(Y, y_predicted)` 计算模型损失。\n",
    "3. 利用 `loss.backward()` 计算模型梯度。\n",
    "4. 利用 `optimizer.step()` 更新权重。\n",
    "5. 利用 `optimizer.zero_grad()` 清空梯度。\n",
    "6. 重复 1-5 的操作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，使用 PyTorch 可以大大的简化我们的编程难度。我们只需要改变模型的形式、损失函数的形式、优化器的形式以及各个参数的值，就能够训练出不同的模型，进而解决不同的深度学习问题了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验详细的阐述了如何使用 PyTorch 对模型进行求解。这个过程既可以适用于传统机器学习问题的求解，也可以适用于神经网络的模型求解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> 本课程内容版权归实验楼所有，禁止转载、下载及非法传播。</i></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
