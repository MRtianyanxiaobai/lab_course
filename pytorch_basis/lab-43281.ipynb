{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 激活函数及可视化"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数是深度学习中很重要的一个概念。在神经网络中，我们经常使用线性运算来解决线性问题。但是日常生活中的大多数问题，都不是简单的线性运算。为此，我们引入了激活函数来解决非线性的问题。本实验主要讲解了深度学习中常用的激活函数的各种形式以及如何利用 PyTorch 对其进行实现。最后利用学到的激活函数，建立了一个简单的三层神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Sigmoid 函数\n",
    "- ReLU 函数\n",
    "- Tanh 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 激活函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "激活函数是深度学习中一个很重要的概念。在神经网络中，我们经常使用线性运算来解决线性问题。但是日常生活中的大多数问题，都不是简单的线性问题。为此，我们引入了激活函数来解决非线性的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "简单的说，传统的全连接网络就是让数据不断的通过线性函数层和激活函数层，进而得到最终的预测结果。因此，如果我们学习完激活函数后，就可以结合之前学过的线性函数来完成一个传统的全连接网络模型了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "常见的激活函数有 Sigmoid 函数（又名 Logistic 函数）、tanh 函数（又名双曲正切函数），ReLU 函数（又名线性修正单元函数）等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sigmoid 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid 函数是深度学习发展中最经典的且最先被使用的激活函数之一。它的公式如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\sigma(z) = \\frac{1}{1+e^{-z}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 $z$ 表示函数的输入，$\\sigma$ 表示函数的输出。根据公式，我们可以画出相关的几何图像："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# 手写 sigmoid 函数\n",
    "sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 画图\n",
    "y=np.linspace(-10,10,100)\n",
    "plt.plot(y,sigmoid(y),'b')\n",
    "plt.grid(linestyle='--')\n",
    "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "plt.yticks([0,0.5, 1])\n",
    "plt.ylim(0, 1)\n",
    "plt.xlim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中可以看到，激活函数 Sigmoid 在定义域内处处可导。但是，通过曲线的斜率，可以发现，当输入一个较小或较大的数时，该函数的导数会变得很小，梯度趋近于 0 。 举个例子，每一次的梯度值都减少 0.25，如果神经网络的隐含层过多，那么当梯度穿过多层后将变得非常接近于 0（可以理解为 0.5 的 n 次方），即出现梯度消失的现象，进而造成模型无法收敛。这也就是为什么，曾经被广泛使用的 Sigmoid 函数，现在却消声觅迹的原因。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "除了自定义 Sigmoid 函数之外，我们还可以通过 PyTorch 对其进行定义："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "# pytorch 中有两种实现方法\n",
    "x = torch.tensor([-1.0, 1.0, 2.0, 3.0])\n",
    "output = torch.sigmoid(x)\n",
    "print(output)\n",
    "s = nn.Sigmoid()\n",
    "output = s(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tanh  函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh 是双曲函数中的双曲正切函数。在数学中，双曲正切函数都是由双曲正弦函数和双曲余弦函数推导而来。函数的具体形式如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$tanh(x)=\\frac{e^x-e^{-x}}{e^x+e^{-x}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanh 的函数图像如下:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tanh = lambda x: 2*sigmoid(2*x)-1\n",
    "\n",
    "y=np.linspace(-10,10,100)\n",
    "plt.plot(y,tanh(y),'b')\n",
    "plt.grid(linestyle='--')\n",
    "plt.xlabel('X Axis')\n",
    "plt.ylabel('Y Axis')\n",
    "plt.xticks([-4, -3, -2, -1, 0, 1, 2, 3, 4])\n",
    "plt.yticks([ -1, 0, 1])\n",
    "plt.ylim(-1, 1)\n",
    "plt.xlim(-4, 4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图可以看出，双曲正切函数和 Sigmoid 函数图像很相似，但是 Tanh 函数的范围为（-1,1），而 Sigmoid 函数的范围为 （0,1）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同 Sigmoid 类似，Pytorch 中也有两种方式实现 tanh："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.tanh(x)\n",
    "print(output)\n",
    "t = nn.Tanh()\n",
    "output = t(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ReLU 函数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "双曲正切函数和 Sigmoid 函数相似，也存在着梯度消失现象。且由于解析式中存在幂运算，计算机需要消耗大量的时间成本。因此，为了解决梯度消失的问题，线性修正单元函数（Rectified Linear Units，简称ReLU）孕育而生。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReLU 函数是目前最常用的激活函数之一。公式如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$f(x)=\n",
    "\\begin{cases}\n",
    "0& \\text{x $<$ 0}\\\\\n",
    "z& \\text{x $\\geq$ 0}\n",
    "\\end{cases}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其中 z 为常数。可以看到，当 $z<0$ 时，ReLU 全部取值为 0，梯度也为 0，减少了梯度的运算成本（这种现象称为硬饱和）。当 $z\\geq 0$ 时，ReLU 的取值为 $z$， 梯度始终为一个固定的值，进而缓解了梯度消失的问题。其函数图像如下所示："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = lambda x: np.where(x>=0, x, 0)\n",
    "\n",
    "y=np.linspace(-10,10,1000)\n",
    "plt.plot(y,relu(y),'b')\n",
    "plt.grid(linestyle='--')\n",
    "\n",
    "plt.xticks([-3, -2, -1, 0, 1, 2, 3 ])\n",
    "plt.yticks([ 0, 1,2,3])\n",
    "plt.ylim(0, 3)\n",
    "plt.xlim(-3, 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "利用 PyTorch 中的 ReLU 函数处理输入数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.relu(x)\n",
    "print(output)\n",
    "relu = nn.ReLU()\n",
    "output = relu(x)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "至此我们学习完了深度学习中常见的几种激活函数。当然，上一个试验中学到的 Softmax 函数也算一个激活函数，该函数主要应用于多分类模型的神经网络的最后一层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们需要解决的是二分类问题，我们一般会将最后一层设置为 Sigmoid 函数层。因为，从 Sigmoid 函数图像可以看出，该函数的范围为 （0,1），这样可以很好的表示概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "综上，考虑到每种激活函数的特性，我们得到了以下的规则：\n",
    "\n",
    "- 如果在神经网络内部（隐含层）需要使用激活函数，一般会使用 ReLU 函数或者 ReLU 函数的改进来进行激活。\n",
    "- 如果是二分类问题，那么会在神经网络的最后一层加上一个 Sigmoid 函数层。\n",
    "- 如果是多分类问题，那么会在神经网络的最后一层加上一个 Softmax 函数层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的建立"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "神经网络其实就是有很多各线性函数和非线性函数组成的复杂函数。至此，我们已经学习完了线性函数和非线性函数的定义方式。接下来，让我们根据这些知识点建立一个完整的神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在利用 PyTorch 建立神经网络模型时，需要注意下面几个点："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 自定的神经网络类必须继承 `nn.Module`。\n",
    "- 自定义类中需要实现 `__init__` 和 `forward` 函数。\n",
    "- `__init__` ： 定义网络的结构。\n",
    "-  `forward`：定义数据在模型中的传播路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 自定的神经网络类必须继承 `nn.Module`。\n",
    "class NeuralNet(nn.Module):\n",
    "    # init 函数主要用于定义，神经网络中有那些结果\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    # 将输出传入网络模型\n",
    "    def forward(self, x):\n",
    "        out = self.linear1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.linear2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "# 测试代码\n",
    "model = NeuralNet(10,20)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从结果可以看出，我们建立的神经网络结构为：连接层（线性函数层）-> ReLU 层 -> 连接层 -> Sigmoid 函数层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果中间有多个 ReLU 函数层，那么按照上面的思路，我们就需要定义 relu1，relu2 ... 等多个变量。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然在定义激活层时，我们可以将其放在 `forward()` 函数中，这样可以减少变量的定义，如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_size, hidden_size)\n",
    "        self.linear2 = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    # 将激活函数层直接放到 forward 中\n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.linear1(x))\n",
    "        out = torch.sigmoid(self.linear2(out))\n",
    "        return out\n",
    "# 测试代码\n",
    "model = NeuralNet(10,20)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然，这样就无法输出 激活函数层的结构了。因为激活函数层被放在了 `forward()` 中，只有调用 forward() 函数时（即传入数据时），计算机才知道还需要进行激活处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验首先讲述了激活函数的主要形式以及用处，然后利用 PyTorch 对每种激活函数进行了实现与应用，最后利用前面所学知识建立了一个简单的神经网络模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> 本课程内容版权归实验楼所有，禁止转载、下载及非法传播。</i></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
