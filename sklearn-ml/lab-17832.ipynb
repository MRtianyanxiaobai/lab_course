{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证和模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前面的实验我们已经学习了大部分监督学习算法和无监督学习算法的 sciki-learn 实现，在本节中，我们将研究模型评估和超参数的调整。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型验证\n",
    "- 交叉验证\n",
    "- 验证曲线\n",
    "- 学习曲线"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 模型验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "机器学习中最重要的部分之一就是模型验证：即检查模型对给定数据集的适应程度。这里选择前面实验中用过的手写数字数据集，来看看如何检查模型对数据的拟合程度："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_digits  # 导入数据集\n",
    "%matplotlib inline\n",
    "\n",
    "plt.style.use('seaborn')  # 样式美化\n",
    "\n",
    "digits = load_digits()  # 加载数据集\n",
    "X = digits.data\n",
    "y = digits.target\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们拟合一个 KNN（K 近邻）分类器："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier  # 导入KNN估计器\n",
    "\n",
    "knn = KNeighborsClassifier(n_neighbors=1)  # 选取最近的点的个数 1\n",
    "knn.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们使用此分类器来预测数据的标签并检查预测的效果如何："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X)  # 得到预测标签\n",
    "print(\"{0} / {1} correct\".format(np.sum(y == y_pred), len(y)))  # 查看预测效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这结果似乎很完美，预测准确率达到了 100%，但实际上我们犯了个错误，那就是,训练数据和测试数据为同一组数据。这并不是一个好方法，如果我们以这种方法优化估计器，我们将过拟合。也就是说，我们会过度学习训练数据，包括学习训练集中的噪声。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试模型的更好方法是使用没有参加训练的数据集。在使用 scikit-learn 的 `train_test_split` 工具的时候，我们已经用到了这个方法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 切分数据集 70%用于训练，30%用于验证\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "X_train.shape, X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们根据训练数据进行训练，并根据测试数据进行验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "knn.fit(X_train, y_train)  # 模型拟合（训练）\n",
    "y_pred = knn.predict(X_test)  # 得到预测标签\n",
    "print(\"{0} / {1} correct\".format(np.sum(y_test == y_pred), len(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这使我们对模型的运行方式有了更可靠的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在这里使用的度量标准，将匹配数与样本总数进行比较，称为准确性，可以使用以下方法进行计算："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score  # 导入评估函数\n",
    "\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实，我们也可以直接从 `model.score` 方法计算得出："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用此方法，我们可以查看随着模型参数（在这种情况下为选取最近的点的个数）的变化，准确性是如何变化的："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_neighbors in [1, 5, 10, 20, 30]:\n",
    "    knn = KNeighborsClassifier(n_neighbors)\n",
    "    knn.fit(X_train, y_train)\n",
    "    print(n_neighbors, knn.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看到在这种情况下，选取的最近的点的个数（n_neighbors）较少似乎是最佳选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证集的一个问题是我们“丢失”了一些数据。在上面的步骤中，我们仅将 3/4 的数据用于训练，而将 1/4 的数据用于验证。我们还有另一个选择，使用 2  折交叉验证，即我们将样本分成两半，然后执行两次验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 平分数据集为两半\n",
    "X1, X2, y1, y2 = train_test_split(X, y, test_size=0.5, random_state=0)\n",
    "print(X1.shape, X2.shape)\n",
    "\n",
    "# 将数据进行两次验证，即第二次验证集为上一次的测试集\n",
    "print(KNeighborsClassifier(1).fit(X2, y2).score(X1, y1))\n",
    "print(KNeighborsClassifier(1).fit(X1, y1).score(X2, y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2 折交叉验证为我们提供了准确性的两个估计，上面是我们自己写的 2 折交叉验证过程，scikit-learn 中有个函数可以直接执行："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score  # 导入交叉验证函数\n",
    "\n",
    "# cv=2 即 2 折交叉验证\n",
    "cross_val_score(KNeighborsClassifier(1), X, y, cv=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K 折交叉验证"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面，我们使用了 2 折交叉验证。这只是 $K$ 折交叉验证的一种，在交叉验证中，我们还可以将数据分成 $K$ 块并执行 $K$ 次拟合，其中每块都作为验证集轮流进行。我们可以通过更改上面的 `cv` 参数来实现。让我们试试 10 折交叉验证："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_val_score(KNeighborsClassifier(1), X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "返回值为每次交叉验证运行得到的准确性的数组。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 过拟合、欠拟合和模型选择"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经了解了验证和交叉验证的基础知识，是时候进一步深入探讨模型选择了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "与验证和交叉验证相关的问题是机器学习实践中很最重要的部分，而为数据选择最佳模型则至关重要。那么当我们的估计器表现得并不好时，我们应该怎么改进呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 使用更简单或更复杂的模型？\n",
    "- 增加更多训练数据？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "答案通常是违反直觉的。特别是，有时使用更复杂的模型会产生更差的结果。另外，有时增加训练数据不会改善您的结果。确定哪些步骤将改善您的模型的能力是成功的机器学习从业者与失败者之间的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们将处理一个简单的一维回归问题。我们可以轻松地可视化数据和模型，并将结果推广到高维数据集。我们使用 `sklearn.linear_model` 模块来完成。首先，创建一个我们想要拟合的简单非线性函数："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_func(x, err=0.5):\n",
    "    y = 10 - 1. / (x + 0.1)\n",
    "    if err > 0:\n",
    "        y = np.random.normal(y, err)  # 生成均值为y，标准差为err的正态分布\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，用上面的非线性函数生成数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_data(N=40, error=1.0, random_seed=1):\n",
    "    np.random.seed(1)\n",
    "    X = np.random.random(N)[:, np.newaxis]  # 生成40个0-1的随机数并增加一个维度\n",
    "    y = test_func(X.ravel(), error)  # X.ravel()又将X变成一维数组\n",
    "\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X, y = make_data(40, error=1)\n",
    "plt.scatter(X.ravel(), y)  # 绘制散点图"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们对该数据进行回归。使用内置的线性回归估计器来计算拟合："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error  # 导入均方误差回归损失计算函数\n",
    "from sklearn.linear_model import LinearRegression  # 导入线性回归估计器\n",
    "X_test = np.linspace(-0.1, 1.1, 500)[:, None]  # 生成 500 个测试数据\n",
    "\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X, y)  # 拟合模型\n",
    "y_test = model.predict(X_test)  # 得到预测值\n",
    "plt.scatter(X.ravel(), y)  # 绘制原数据散点图\n",
    "plt.plot(X_test.ravel(), y_test)  # 绘制预测线\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(\n",
    "    mean_squared_error(model.predict(X), y)))\n",
    "# 计算均方误差回归损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经对数据拟合了一条直线，但是显然此模型并不适合此数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们尝试通过创建更复杂的模型来改善这一点。我们可以通过添加自由度并在输入上计算多项式回归来实现。scikit-learn 通过 [PolynomialFeatures](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) 多项式特征生成器使此操作变得容易。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures  # 导入多项式特征生成器\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline  # 导入算法串联\n",
    "\n",
    "# 将 2 次多项式与线性函数串联起来\n",
    "\n",
    "\n",
    "def PolynomialRegression(degree=2, **kwargs):\n",
    "    return make_pipeline(PolynomialFeatures(degree),\n",
    "                         LinearRegression(**kwargs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们将使用它来用二次曲线拟合数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(2)  # 创建二次曲线模型\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)  # 得到预测值\n",
    "\n",
    "plt.scatter(X.ravel(), y)  # 绘制训练数据\n",
    "plt.plot(X_test.ravel(), y_test)  # 绘制预测曲线\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(\n",
    "    mean_squared_error(model.predict(X), y)))\n",
    "# 计算均方误差回归损失"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这样可以减少均方误差，并且拟合效果更好。我们再看看用更高阶的多项式的效果："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(30)  # 创建含 30 次多项式的模型\n",
    "model.fit(X, y)\n",
    "y_test = model.predict(X_test)\n",
    "\n",
    "plt.scatter(X.ravel(), y)\n",
    "plt.plot(X_test.ravel(), y_test)\n",
    "plt.title(\"mean squared error: {0:.3g}\".format(\n",
    "    mean_squared_error(model.predict(X), y)))\n",
    "plt.ylim(-4, 14)  # 设置 y 轴范围"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们将多项式次数提高到这种程度时，很明显，拟合结果不再反映真实的分布，而是对训练数据中的噪声更加敏感。因此，我们称其为高方差模型，并且说它过度拟合了数据，即过拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以用 `interact` 交互功能来看看随着训练数据数量和多项式次数的变化，拟合曲线的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import interact\n",
    "\n",
    "# 初始数据数为50，多项式为一次\n",
    "\n",
    "\n",
    "def plot_fit(degree=1, Npts=50):\n",
    "    X, y = make_data(Npts, error=1)  # 生成 Npts 个数据\n",
    "    X_test = np.linspace(-0.1, 1.1, 500)[:, None]\n",
    "\n",
    "    model = PolynomialRegression(degree=degree)\n",
    "    model.fit(X, y)  # 拟合模型\n",
    "    y_test = model.predict(X_test)\n",
    "\n",
    "    plt.scatter(X.ravel(), y)  # 绘制数据散点图\n",
    "    plt.plot(X_test.ravel(), y_test)  # 绘制预测曲线\n",
    "    plt.ylim(-4, 14)\n",
    "    plt.title(\"mean squared error: {0:.2f}\".format(\n",
    "        mean_squared_error(model.predict(X), y)))  # 计算均方误差回归损失\n",
    "\n",
    "\n",
    "#\n",
    "interact(plot_fit, degree=(1, 30), Npts=(2, 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 验证曲线检测过拟合"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "显然，仅计算训练数据上的均方误差是不够的。如上所述，我们可以使用交叉验证来更好地判断模型拟合，并绘制验证曲线直观看看拟合效果。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为了使结果更明显，我们将使用稍大的数据集："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 生成 120 个非线性数据\n",
    "X, y = make_data(120, error=1.0)\n",
    "plt.scatter(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们使用 `validation_curve` 函数来实现验证曲线的绘制，代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import validation_curve  # 导入验证曲线函数\n",
    "\n",
    "# 均方误差计算\n",
    "\n",
    "\n",
    "def rms_error(model, X, y):\n",
    "    y_pred = model.predict(X)\n",
    "    return np.sqrt(np.mean((y - y_pred) ** 2))\n",
    "\n",
    "\n",
    "# 设置多项式次数\n",
    "degree = np.arange(0, 18)\n",
    "# 以均方误差计算训练集和测试集得分\n",
    "val_train, val_test = validation_curve(PolynomialRegression(), X, y,\n",
    "                                       'polynomialfeatures__degree', degree, cv=7,\n",
    "                                       scoring=rms_error)\n",
    "# 绘制训练和验证曲线\n",
    "\n",
    "\n",
    "def plot_with_err(x, data, **kwargs):\n",
    "    mu, std = data.mean(1), data.std(1)\n",
    "    lines = plt.plot(x, mu, '-', **kwargs)\n",
    "    plt.fill_between(x, mu - std, mu + std, edgecolor='none',\n",
    "                     facecolor=lines[0].get_color(), alpha=0.2)  # 填充\n",
    "\n",
    "\n",
    "plot_with_err(degree, val_train, label='training scores')\n",
    "plot_with_err(degree, val_test, label='validation scores')\n",
    "plt.xlabel('degree')\n",
    "plt.ylabel('rms error')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请注意此处的趋势，这是此类图的常见趋势。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 模型复杂度较小时，训练误差和验证误差非常相似。这表明模型不适合此数据：它没有足够的复杂度来表示数据。换句话说，这是一个高偏差模型。\n",
    "- 随着模型复杂度的增加，训练和验证分数也会有所不同。这表明该模型过拟合了：它具有很大的灵活性，适合噪声而不是潜在趋势。换句话说，这是一个高方差模型。\n",
    "- 验证数据通常有一个最佳点，这里大概在 5 左右。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据交叉验证，这就是我们最合适的模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PolynomialRegression(4).fit(X, y)  # 4 次多项式模型\n",
    "plt.scatter(X, y)\n",
    "plt.plot(X_test, model.predict(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 学习曲线检测数据充足性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "不难猜到，偏差和方差之间的确切转折点与训练数据量有关。在这里，我们将说明学习曲线的用法，该曲线显示了不同数据量下的学习得分。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据训练点数绘制训练和测试集的均方误差："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve  # 导入学习曲线函数\n",
    "\n",
    "\n",
    "def plot_learning_curve(degree=3):\n",
    "    train_sizes = np.linspace(0.05, 1, 120)\n",
    "    # 得到5折交叉验证后的训练示例数、训练集得分和验证集得分\n",
    "    N_train, val_train, val_test = learning_curve(PolynomialRegression(degree),\n",
    "                                                  X, y, train_sizes, cv=5,\n",
    "                                                  scoring=rms_error)\n",
    "    plot_with_err(N_train, val_train, label='training scores')  # 绘制训练得分\n",
    "    plot_with_err(N_train, val_test, label='validation scores')  # 绘制验证得分\n",
    "    plt.xlabel('Training Set Size')\n",
    "    plt.ylabel('rms error')\n",
    "    plt.ylim(0, 3)  # 设置参数范围\n",
    "    plt.xlim(5, 80)\n",
    "    plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们看看 degree=1 时，即线性模型的学习曲线："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这显示了一条典型的学习曲线：对于很少的训练点，训练误差和测试误差之间存在很大的距离，这表明过拟合。给定相同的模型，对于大量的训练点，训练和测试误差收敛，这表明欠拟合。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随着数据量的增加，训练误差将不会增加，测试误差也将不会减少。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可以很容易地看出，在此图中，如果您希望将 MSE 降低到标准值1.0（这是我们在构建数据时输入的分散量的大小），样本数的增加并没有什么用。对于 degree=1，两条曲线已经收敛并且不能向下移动。 那么我们增加 $ degree $ 的值呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从上图结果看到，通过增加更多的模型复杂度，我们可以将收敛水平降低到均方误差为 1.0。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那如果模型复杂度再进一步提高呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_learning_curve(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于一个更复杂的模型，仍然会收敛，但是收敛要求的数据量越来越大。因此："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 可以通过增加数据量或简化模型来使曲线收敛。\n",
    "- 只能通过增加模型的复杂度来减小收敛误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习曲线告诉我们如何优化模型。如果曲线已经很靠近，则需要更多的模型复杂度。如果曲线相距较远，则还可以通过增加数据量来改进模型。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验，我们学习了参数的优化、交叉验证和验证曲线、学习曲线等模型评估工具，这些工具是评估数据模型的有力手段。至此，本课程也已全部结束，希望你能有所收获。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> 本课程内容版权归实验楼所有，禁止转载、下载及非法传播。</i></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
