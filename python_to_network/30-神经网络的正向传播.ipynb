{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 神经网络的正向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验追踪输入信号，详细的描述了，信号在整个神经网络中的传播过程。然后学习了矩阵相关运算，将神经网络的正向传播矩阵话。最后，举了一个三层神经网络的例子，加深了对神经网络正向传播的理解。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 信号的传播\n",
    "- 矩阵的乘法\n",
    "- 矩阵在神经网络中的应用\n",
    "- 信号在多层神经网络中的传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 神经网络的工作原理"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上一个实验中的三层神经元结构图，看起来让人相当惊奇。但是，信号如何经过一层一层的神经元，从输入变成输出，这个复杂的过程却令人生畏。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是这对说明神经网络的工作原理非常重要。因此，我们尝试使用只有两层，且每层只有两个神经元的较小的神经网络，来演示神经网络的工作原理，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191209-1575878953113/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们想象一下，两个输入值分别为 $1.0$ 和 $0.5$ 。将这些值输入到这个较小的神经网络，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191209-1575879191647/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每个节点使用激活函数，将输入转变成输出。这里的激活函数使用之前学习到的 S 函数：$ y = \\frac{1}{1 + e^{−x} }$。其中神经元输入信号的总和为 $x$，神经元的输出为 $y$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "权重是什么？权重的初始值应该为多少？这是一个很好的问题。这里我们可以先随机初始化权重，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ w_{1,1} = 0.9 \n",
    "，w_{1,2} = 0.2 \\\\\n",
    " w_{2,1} = 0.3 ，\n",
    "w_{2,2} = 0.8 $$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "随机初始值是个不错的主意，在之前求解线性分类器的斜率时，我们采用的也是这样一种方法。随着分类器学习各个样本，随机值就可以得到改进。对于神经网络链接的权重而言，也是一样的道理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们在网络结构中将上述的初始权重标识出来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"350px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191209-1575879711501/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们开始计算吧！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一层节点是输入层，这一层不做其他事情，仅表示输入信号。也就是说，输入节点不对输入值应用激活函数。这没有什么其他奇妙的原因，自然而然地，历史就是这样规定的。也就是说输入层就是输入，无需任何处理。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是第二层，其中第二层主要分为两部分。首先通过输入和权重相乘来计算总和输入值。再将总和输入值，传入我们之前定义的 S 阈值函数中，最后得到该层神经元的输出。具体过程如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191209-1575883743965/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，首先让我们关注**第二层的 1 节点**。第一层中的两个节点连接到了这个节点。这些输入节点的原始值为：1.0 和 0.5 。第一层的第一个节点链接值为 0.9 的权重，第一层的第一个节点链接值为 0.3 的权重。因此，组合经过了权重调节后的输入，如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}  x &=& (第一个节点的输出 \\times 链接权重) + (第二个节点的输出 \\times 链接权重)\\\\\n",
    "   &=&(1.0 \\times 0.9)+(0.5 \\times 0.3)\n",
    "  \\\\ &=& 0.9 + 0.15 \n",
    "  \\\\ &=& 1.05 \n",
    "  \\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们已经得到了 $x=1.05$ ，这是第二层第一个节点的组合调节输入。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最终，我们再可以使用激活函数 $y = \\frac{1}{1 + e^{−x} }$ 计算该节点的输出。你可以使用计算器来进行这个计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1} {1 + 0.3499}=\\frac{ 1} { 1.3499} = 0.7408$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们得到了神经网络中两个输出节点的实际输出中的一个。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们继续计算剩余的节点，即第二层第二个节点。激活函数的输入 $x$ 为： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\begin{eqnarray}  x &=& (第一个节点的输出 \\times 链接权重) + (第二个节点的输出 \\times 链接权重)\\\\\n",
    "   &=&(1.0 \\times 0.2)+(0.5 \\times 0.8)\n",
    "  \\\\ &=& 0.2 + 0.4 \n",
    "  \\\\ &=& 0.6\n",
    "  \\end{eqnarray}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，现在我们可以使用 S 激活函数:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$y = \\frac{1}{1 + 0.5488} = \\frac{1}{1.5488} = 0.6457$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示了我们刚刚计算得到的网络输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191209-1575885277047/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从一个非常简化的网络得到两个输出值，这个工作量相对较小。对于一个相对较大的网络，我不希望使用手工进行计算！好在计算机在进行大量计算方面表现的非常出色，并且不知疲倦和厌烦。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "即便如此，对于具有多于两层，每一层有 4、8 甚至 100 个节点的网络，我也不希望编写计算机指令来对这样的网络进行计算。即使只是写出所有层次和节点的计算指令，也会让我感到枯燥，让我犯错，更不用说手工进行这些计算了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好在，即使是面对一个具有很多层、众多节点的神经网络，数学可以帮助我们以非常简洁的方式写下计算出所有输出值的指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一简洁方法就是使用矩阵，接下来，我们就来看看矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 矩阵的运算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵有着让人闻风丧胆的声誉。它们唤起了我们的记忆，让我们记起在学校进行矩阵乘法时，那种让人咬牙切齿、枯燥费力的工作，以及那毫无意义的时间一小时一小时地流逝。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前，我们手工对两层的神经网络进行计算。对人类而言，这样的工作量也是足够大了，但是，请你想象一下，我们要对五层、每层100个节点的网络进行相同的计算，这是一种什么感受？单单是写下所有必要的计算，也是一个艰巨的任务……对每一层每一个节点，计算所有这些信号的组合，乘以正确的权重，应用S激活函数……"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，矩阵如何帮助我们简化计算呢？其实，矩阵在以下两个方面给与了我们帮助。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 矩阵允许我们压缩所有这些计算，把它们变成一种非常简单的缩写形式。\n",
    "- 许多计算机编程语言理解如何与矩阵一起工作，计算机编程语言能够认识到实际的工作是重复性的，因此能够高效高速地进行计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "总之，矩阵允许我们简洁、方便地表示我们所需的工作，同时计算机也可以快速高效地完成计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们开始使用矩阵，揭开矩阵的神秘面纱。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵其实就是一个数字表格、矩形网格而已。对于矩阵而言，没有更多复杂的内容了。如下图，是一个 2 行 3 列的数字网格，同样也是一个大小为 $2\\times3$ 的矩阵（其中第一个数字代表行数，第二个数字代表列数）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575941806295/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此外，一些人使用方括号表示矩阵，另一些人与我们一样，使用圆括号表示矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实，矩阵的元素也不必是数字，它们也可以是我们命名的、但还未赋予实际的数值的一个量。因此，以下是这样一个矩阵：每个元素都是一个变量，具有一定的意义。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942068953/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵对我们很有用处，让我们看看它们是如何相乘的。你可能还记得我们在学校时是如何进行矩阵计算的。如果你还没有这种经历，让我们再演示一遍。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面是两个矩阵相乘的一个示例。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942148721/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以观察到，我们并不是简单地将对应的元素进行相乘。左上角的答案不是 $1\\times5$，右下角的答案也不是 $4\\times8$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "相反，矩阵使用不同的规则进行乘法运算。通过观察上面的示例，你可以得出这些规则。如果你不能总结出这些规则，那么请仔细观察下图，下图中突出显示了答案中左上角的元素是如何计算得出的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942190542/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实很简单，输出的第一行第一列的的元素值，乘法左边矩阵的第一行的所有值与乘法右边矩阵的第一列的所有值，然后求和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "换句话说，若 $A\\cdot B=C$，则 $C_{i,j}$ 为 $A$ 的第 $i$ 行的所有值，乘以 B 的第 $j$ 列的所有值的和。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们用输出矩阵的右下角元素来举例子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在下图中，右下角元素所在位置为第 2 行，第 2 列。因此该元素的值应该等于等号左边第一个矩阵的第 2 行的所有值（即$3,4$）乘以等号左边第二个矩阵的第 2 列的所有值（即 $6,8$）。因此右下角元素的计算公式为：$(3\\times 6 )+(4\\times  8)=50$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942361755/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "同理，左下角的元素计算公式为 $(3 \\times 5)+(4 \\times 7)= 15 + 28 = 43$。同样地，右上角元素的计算公式为$(1 \\times 6)+(2 \\times 8)= 6 + 16 = 22$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面我们使用变量，来代表上面的规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942391191/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用代表任意数字的字母，我们可以更加清晰地理解矩阵相乘的一般规则。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当然也并不是说任意两个矩阵都可以进行矩阵相乘。上面已经很详细的阐述了，在式子 $A\\cdot B=C$ 中，$C_{i,j}$ 为 $A$ 的第 $i$ 行的所有值，乘以 B 的第 $j$ 列的所有值的和。显而易见，如果 A 的第 $i$ 行有 5 个元素，B 的第 j 列有 3 个元素，那么他们就无法相乘，因为个数不匹配。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此两个矩阵可以相乘的必要条件是：第一个矩阵的行数等于第二个矩阵的列数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一些书籍将这样的矩阵乘法称为点乘（Dot Product）或内积（Inner Product）。实际上，对于矩阵而言，还有其他类型的乘法，如叉乘，但是我们此处所指的是点乘。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么我们要讨论让人望而生畏的矩阵乘法和令人反感的代数呢？这看起来像是一个“无底洞”。这里有一个非常重要的理由……我们先暂且不提！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请仔细观察，如果我们将字母换成对神经网络更有意义的单词，那么会发生什么情况呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575942444240/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个矩阵包含两层节点之间的权重。第二个矩阵包含第一层输入层的信号。通过两个矩阵相乘，我们得到的答案是输入到第二层节点组合调节后的信号。仔细观察，你就会明白这点。由权重 $w_{1,1}$ 调节的 input_1 加上由权重 $w_{2,1}$调节的 input_2，就是第二层第一个节点的输入值。这些值就是应用在 $S$ 函数中的 $x$ 的值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图非常清楚地显示了这个过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575955570494/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这真的非常有用啊！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "为什么呢？因为我们可以使用矩阵乘法一次性计算出所有组合调节后的信号 $x$，并且输入到第二层的节点中。我们可以使用下式，非常简洁地表示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X = W \\cdot I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处，$W$ 是权重矩阵，$I$ 是输入矩阵，$X$ 是组合调节后的信号集合，即输入到第二层的结果矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，我们不需要在乎每一层有多少个节点。不管 $I$ 有 $2$ 个元素还是有 $200$ 个元素,我们都可以简单地写为 $W \\cdot I$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，如果计算机编程语言可以理解矩阵符号，那么计算机就可以完成所有这些艰辛的计算工作，算出 $X = W \\cdot I$，而无需我们对每一层的每个节点给出单独的计算指令。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这真是太棒了！只要努力一点，理解矩阵乘法，就可以找到如此强大的工具，这样我们无需花费太多精力就可以实现神经网络了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "有关激活函数，我们该了解些什么呢？激活函数其实很简单，并不需要矩阵乘法。我们所需做的，是将矩阵 $X$ 中的每个元素应用于 $S$ 函数 $y = \\frac{1}{1 + e^{-x}} $ 中 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，来自第二层的最终输出是："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$O = sigmoid (X)= \\frac{1}{1 + e^{-X}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "斜体的 $O$ 代表矩阵，这个矩阵包含了来自神经网络的最后一层中的所有输出。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "表达式 $X = W \\cdot I$ 不仅适用于第一层和第二层，而且还适用于任意两层之间的计算。比如说，在计算第三层的 X 时，只需把第二层的输出作为第三层的输入，重复上面的操作即可。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们使用学到的矩阵乘法相关知识，去理解，三层神经网络中，信号的传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 三层神经网络示例"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示了一个 3 层的、且每层具有 3 个节点的神经网络结构。为了保证图的清晰，我们并没有标上所有的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575956609523/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们所知道的，第一层为输入层，最后一层为输出层，中间层我们称之为隐藏层（因为中间层的输出不需要很明显地展示出来，因此称之为“隐藏”层）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先，我们观察到了 $3$ 个输入是 $0.9$、$0.1$ 和 $0.8$。因此，输入矩阵 $I$ 为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width= \"200px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575957271326/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因为输入层所做的事情就是输入，因此我们已经完成了第一层输入层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来是中间的隐藏层。在这里，我们需要计算出输入到中间层每个节点的信号组合。请记住，中间隐藏层的每个节点都与输入层的每个节点相连，因此每个节点都得到输入信号的部分信息。我们无需想先前那样做大量的计算，我们希望尝试矩阵的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "正如我们刚才看到的，输入到中间层的组合信号为 $X = W \\cdot I$ ，其中 $I$ 为输入信号矩阵，$W$ 为权重矩阵。当表示矩阵时，我们一般使用大写字母，表示矩阵中的某个元素时，我们使用小写字母。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个神经网络的 $I$、$W$ 是什么样的呢？图中显示了这个神经网络的一些权重，但是并没有显示所有的权重。下图显示了所有的权重，同样，这些数字是**随机**列举的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575957548916/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，第一个输入节点和中间隐藏层第一个节点之间的权重为 $w_{1,1} = 0.9$ 。同样，你可以看到输入层的第二节点和隐藏层的第二节点之间的链接的权重为 $w_{2,2} = 0.8$ 。由于图中并没有显示输入层的第三个节点和隐藏层的第一个节点之间的链接，我们随机编了一个权重 $w_{3,1} = 0.4$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们用 $W_{input\\_hidden}$ 表示输入层和隐藏层之间的权重。用 $W_{hidden\\_output}$ 表示隐藏层和输出层之间的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示的是第二个矩阵 $W_{hidden\\_output}$ ，如先前一样，在矩阵中填写了权重。举个例子，同样你可以看到，隐藏层中的第三个节点和输出层中的第三个节点之间的权重为 $w_{3,3} = 0.9$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575958844580/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "太棒了，我们已经得到了排列整齐的权重矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们继续计算输入到隐藏层的组合输入值。我们应该给这个输入值一个名称，考虑到这个组合输入是到中间层的，而不是最终层的。因此，我们将它称为 $X_{hidden}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ X_{hidden} = W_{input\\_hidden} \\cdot I$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "矩阵乘法的运算结果，如下："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img  width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575961485602/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处我们无需手动的进行矩阵的乘法运算，因为那正是计算机的用武之地。在后面的章节，我们将会一同学习如何使用 Python 编程语言进行这项工作。现在，我们不希望因为计算机软件而分心，因此我们暂时不进行这个工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们已经拥有了从输入到中间隐藏层的组合输入值 $X_{hidden}$ ，它们为 $1.16$、$0.42$ 和 $0.62$。我们使用矩阵来完成了这种复杂的工作，这是一个值得我们自豪的成就。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们对刚才计算出来的组合输入值进行可视化，如下图："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575961936278/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然到目前为止，一切都还顺利，但是，我们还有更多的工作要做。我们对这些节点应用了 S 激活函数，使得节点对信号的反应更像自然界中节点的反应，你应该记得这一点吧！因此，现在，我们进行这个操作："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ O_{hidden} = sigmoid(X_{hidden}) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $X_{hidden}$ 层中的元素上应用 S 函数，生成中间隐藏层的输出矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575962525665/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们检查第一个元素，确定一下过程。$S$ 函数为 $y = \\frac{1}{1 + e ^{-x}}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，当 $x = 1.16$ 时，$ e^ {-1.16}$ 是 $0.3135$。这意味着 $y = \\frac{1}{1 + 0.3135} = 0.761 $ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会看到，$S$ 函数的值域在 $0$ 和 $1$ 之间，因此所有的输出值都会处在这个区间。如果你回头看看逻辑函数的图形，也可以直观地看到这一点。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "唷！让我们再次暂停，看看我们已经完成的事情。当信号通过中间层时，我们对这些信号进行了计算，也就是说我们计算出了中间层的输出值。让我们更新上面的可视化数据图。如下所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575963605748/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们已经计算出了第二层的输出值，接下来我们还需要计算第三层的输出值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们如何处理通过第三层的信号呢？我们可以采用与处理第二层信号相同的方式进行处理，这没有任何实质的区别。我们仍然可以得到第三层的输入信号，就像我们得到第二层的输入信号一样。我们依然使用激活函数，使得节点的反应与我们在自然界中所见到的一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，需要记住的事情是，不管是神经网络的第 3 层，还是第 5 层，还是第 155 层，我们都应该一视同仁。即先组合输入信号，并利用权重调节这些输入信号，再利用激活函数，生成这些层的输出信号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，让我们扬帆起航，与我们以前所做的一样，继续计算最终层的组合调节输入 $X = W \\cdot I$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这一层的输入信号是第二层的输出信号，也就是我们刚刚解出的 $O_{hidden}$ 。所使用的权重就是第二层和第三层之间的链接权重 $W_{hidden\\_output}$ 。因此，我们得到："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$X_{output} = W_{hidden\\_output} \\cdot  O_{hidden} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用同样的方式对矩阵进行运算，这样我们就得到最后一层（输出层）的组合输入信号 $X_{output}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575966990858/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，更新示意图，展示我们的进展，从初始输入信号开始，前馈信号，并得到了最终层的组合输入信号。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575967269393/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "剩下的工作就是应用 $S$ 激活函数，这是很容易的一件事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575967531310/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图，成功就是这么的容易！我们得到了神经网络的最终输出信号： $0.726$、$0.708$ 和 $0.778$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们成功追踪了神经网络中的信号，从信号进入神经网络，并且通过神经网络的各层的处理，最终得到了输出层的输出信号。如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191210-1575968926083/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "其实上述整个过程就是神经网络的正向传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学习完神经网络正向传播后，我们得到了神经网络一次正向传播的输出值。那么得到输出值后我们又该进行什么操作呢？还记得章节一中寻找线性预测器的斜率的方法吗？下一个章节我们将利用章节一中的方法，将神经网络的输出值与训练样本中的输出值进行比较，计算出误差。然后，根据这个误差值来调整神经网络本身，进而改进神经网络的输出值。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> 本课程内容版权归实验楼所有，禁止转载、下载及非法传播。</i></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
