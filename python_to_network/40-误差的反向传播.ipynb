{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 误差的反向传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 介绍"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "本实验详细的阐述了误差在权重更新时的作用。从单个误差入手，再到多个误差的传播，一步一步深化主题，直到完整的阐述完神经网络的反向传播的全部过程。最后利用矩阵形式，尽量简单的阐述了误差在整个神经网络中的传播过程。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 知识点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 神经网络的权重学习\n",
    "- 反向传播的意义\n",
    "- 输出误差的分割\n",
    "- 反向传播的全过程\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 单误差的传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先前，我们通过调整线性函数的斜率，来调整简单的线性分类器。我们使用误差值，也就是节点生成的答案与已知的正确答案之间的差值，来引导我们进行调整。由于简单的线性分类器，只有一个输入。因此，我们调整权重（即斜率）非常容易。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是，当输出和误差是由多个节点共同作用的结果时，我们又应该如何更新权重呢？下图详细阐释了这个问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576030089785/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一种思想是让所有造成误差的节点来平分误差，如下图所示。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576031334988/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "另一种思想是不等分误差。与前一种思想相反，我们为较大权重的链，分配更多的误差。为什么这样做呢？因为这些链对误差的贡献较大。下图详细阐释了这种思想。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576031517397/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处，有两个节点对输出节点的信号做出了贡献，它们的链接权重分别是 $3.0$ 和 $1.0$。如果按权重比例分割误差，那么输出误差的 $\\frac{3}{4}$ 应该用于更新第一个较大的权重。误差的 $\\frac{1}{4}$ 可以用来更新较小的权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以将同样的思想扩展到多个节点。如果我们拥有 $100$ 个节点链接到输出节点，那么我们要在这 $100$ 条链接之间，按照每条链接对误差所做贡献的比例（由权重大小），来分割误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你可以观察到，我们在两件事情上使用了权重。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 我们使用权重，在神经网络中，将信号从输入向前传播到输出层。也就是我们常说的正向传播。\n",
    "- 我们使用权重，将误差从输出向后传播到网络中。我们称这种方法为反向传播，你应该不会再对此感到惊讶了吧。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果输出层有 $2$ 个节点，那么我们对第二个输出节点也会做同样的事情。第二个输出节点也会有其误差，这个误差也可以通过相连的权重进行类似的分割。在接下来的内容中，我们将来看看这个方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多节点的误差传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示了具有 $2$ 个输入节点和 $2$ 个输出节点的简单网络。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"350px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576032059655/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "你会发现，我们需要使用这两个误差值来告知神经网络如何调整内部的链接权重。我们可以使用与先前一样的方法，也就是按照权重比例，分割输出节点的误差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们拥有多个节点这一事实并没有改变任何事情。对于第二个输出节点，我们只是简单地重复第一个节点所做的事情。为什么如此简单呢？这是因为进入输出节点 2 的权重不依赖与进入输出节点 1 的权重，两者相互独立。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们再次观察此图，我们将第一个输出节点的误差标记为 $e_1$ 。请记住，这个值等于由训练数据提供的正确值 $t_1$ 与实际模型的输出值 $o_1$ 之间的差。也就是，$e_1 = t_1 - o_1 $ 。同理，我们将第二个输出节点的误差标记为 $e_2$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从图中，你可以发现，按照链接的权重比例（即 $w_{1,1}$ 和 $ｗ_{2,1}$ 的比例 ），我们对误差 $e_1$ 进行了分割。类似地，我们按照权重 $w_{1,2}$ 和 $ｗ_{2,2}$ 的比例分割了 $e_2$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们写出这些分割值，这样我们就不会有任何疑问了。我们使用误差 $e_1$ 的信息，来调整权重 $w_{1,1}$ 和 $w_{2,1}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过这种分割方式，我们使用 $e_1$ 的一部分来更新 $w_{1,1}$。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576032877028/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "类似地，用来调整 $w_{2,1}$ 的 $e_1$ 部分为："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"200px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576032885441/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "比如说，$w_{1,1} = 6$ ，$w_{2,1} = 3$，那么用于更新 $w_{1,1}$ 的 $e_1$ 的部分就是 $\\frac{6}{6 + 3}= \\frac{6}{9} = \\frac{2}{3}$ 。同时，这留下了 $\\frac{1}{3}$ 的 $e_1$ 给较小的权重 $w_{2,1}$ 。当然，我们也可以通过表达式 $\\frac{3}{6 + 3}= \\frac{3}{9}$ 来确定分给 $w_{2,1}$的，确实是 $\\frac{1}{3}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在更加深入理解之前，让我们先暂停一下，退后一步，从一个较远的距离观察我们已经做的事情。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们知道需要使用误差来指导在网络内部如何调整一些参数，在这里也就是链接权重。我们明白了如何调整链接权重，并且我们使用链接权重来调节进入神经网络最终输出层的信号。在存在多个输出节点的情况下，我们也看到了这种调整过程没有变得复杂，只是对每个输出节点都进行相同的操作。然后就搞定了！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们要问的问题是，当神经网络多于2层时，会发生什么事情呢？在离最终输出层相对较远的层中，我们如何更新链接权重呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 多网络层的误差传播"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下图显示了具有 $3$ 层的简单神经网络，一个输入层、一个隐藏层和一个最终输出层。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576041524658/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里，我们使用输出层的误差值来引导和调整链接的权重。我们将输出误差标记为 $e_{output}$，将输出层和隐藏层之间的链接权重标记为 $w_{ho}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于额外的新层，我们只需要使用与该隐藏层节点相关联的误差 $e_{hidden}$，并将这些误差按照输入层和隐藏层之间的链接权重 $w_{ih}$ 进行分割即可。下图就显示了此逻辑。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576042478035/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "无论神经网络具有多少层，我们都可以从最终的输出层出发，往回工作，对每一层重复应用相同的思路。这就是误差的反向传播的工作原理。但是，还有一个问题，隐藏层节点的误差 $e_{hidden}$ 的取值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对于输出层节点的 $e_{output}$ ，我们首先使用了输出误差。那么，对于隐藏层节点 $e_{hidden}$ ，我们应该使用什么误差呢？根据上一章节，我们了解到，在向前馈送输入信号时，隐藏层的每个节点都会有一个输出。而且这个输出是通过上一层的信号的加权组合，再通过 S 函数处理，得到的。但是，我们如何应该如何利用这个输出去计算出误差呢？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对于隐藏层的节点，我们没有目标值或所希望的输出值。我们只有最终输出层节点的目标值，这个目标值来自于训练样本数据。让我们再次观察上图，寻找一些灵感！隐藏层第一个节点具有两个链接，这两个链接将这个节点连接到两个输出层节点。我们知道，沿着各个链接可以分割输出误差，就像我们先前所做的一样。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "那么，我们可不可以尝试，把两个来自于同一节点的链接进行误差重组，得到该节点的误差呢？具体流程，如下图所示："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576045662359/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，隐藏层的第一个节点的误差是该节点的所有前向链接的分割误差的和。在上图中，我们得到了权重为 $w_{1,1}$ 的链接上的输出误差 （即 $e_{output ,1} $ 的一部分）。同时也得到了在权重为 $w_{1,2}$ 的链接上的输出误差（即 $ e_{output,2}$  的一部分）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们将这些值写下来。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576050017298/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这有有助于我们看到这个理论的实际作用，下图详细阐释了在一个简单的，具有实际数字的 $3$ 层网络中，误差如何向后传播。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576050325490/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "让我们演示一下反向传播的误差。你可以观察到，第二个输出层节点的误差 $0.5$ ，在具有权重 $1.0$ 和 $4.0$ 的两个链接之间，根据比例被分割成了 $0.1$ 和 $0.4$。你也可以观察到，隐藏层的第二个节点的重组误差等于连接的分割误差之和，也就是 $0.48 + 0.44 = 0.88$。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如下图所示，我们应用相同的思路继续向后工作。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576050821495/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  反向传播的矩阵表示"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来我们将利用矩阵乘法，来简化我们的反向传播运算。顺便说一句，其实这就是将过程进行矢量化（Vectorise The Process）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，让我们回顾一下，上一部分中的神经网络结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"500px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191211-1576045662359/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如上图所示，计算的起始点是在神经网络最终输出层中出现的误差。此时，神经网络的输出层只有两个节点，因此误差只有 $e_1$ 和 $e_2$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191216-1576462918294/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们需要为隐藏层的误差构建矩阵。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果你再次观察上图可以看到，在输出层中，有两条路径对隐藏层的第一个节点的误差做出了“贡献”。沿着这些路径，我们发现了误差信号 $ e_1 \\times\\frac {w_{1,1}}  { w_{1,1} + w_{2,1} }$ 和 $ e_2 \\times \\frac{  w_{1,2}} { w_{1,2} + w_{2,2} }$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们看看隐藏层的第二个节点，同样，我们看到了有两条路径对这个误差做出了“贡献”，我们得到误差信号 $e_1 \\times \\frac {  w_{2,1}} { w_{2,1} + w_{1,1} }$ 和 $ e_2 \\times \\frac { w_{2,2} }{ w_{2,2} + w_{1,2}}$ 。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们得到了下列的隐藏层矩阵。这比想要的矩阵要复杂一些。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"400px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191216-1576464978676/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果这个矩阵能够重写，变成一种我们已知可用、简单的矩阵乘法，那就太棒了。这是权重、前向信号和输出误差矩阵。请记住，如果我们能够重写矩阵，那将是大有裨益的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "遗憾的是，我们不能像先前在处理前馈信号时一样，很容易就将这种矩阵转换为超级简单的矩阵乘法。在这个超级麻烦的大矩阵中，一个个的分数是难以处理的！如果我们能够将这个麻烦的矩阵整齐地分割成简单可用的矩阵组合，就大有益处了。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可以做些什么呢？我们依然非常希望利用矩阵乘法来完成计算。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "再次观察上面的表达式。你可以观察到，最重要的事情是输出误差与链接权重 $w_{ij}$ 的乘法。较大的权重就意味着携带较多的输出误差给隐藏层。这是非常重要的一点。又因为，这些分数的分母是一种归一化因子。如果我们忽略了这个因子，那么我们仅仅失去了后馈误差的大小，但是每个节点得到的反馈误差的比例是不会变的。也就是说，我们使用简单得多的 $e_1 * w_{1,1}$ 来代替 $ e_1 \\times\\frac {w_{1,1}}  { w_{1,1} + w_{2,1} }$  。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果我们采用这种方法，那么矩阵乘法就非常容易辨认。如下所示： "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191216-1576466070364/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个权重矩阵与我们先前构建的矩阵很像，但是这个矩阵沿对角线进行了翻转，因此现在右上方的元素变成了左下方的元素，左下方的元素变成了右上方的元素。我们称此为转置矩阵，记为 $w^T$ 。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "此处，有两个数字转置矩阵的示例，因此，我们可以清楚地观察到所发生的事情。你可以看到，即使矩阵的行数和列数不同，也是可以进行转置的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191216-1576466275296/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "因此，我们将上面的 $error_{hidden}$ 的函数式用矩阵变量的形式表示出来："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"300px\" src=\"https://doc.shiyanlou.com/courses/uid1166617-20191216-1576466866626/wm\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "虽然这样做看起来不错，但是将归一化因子切除的这个操作，是否正确呢？实践证明，这种相对简单的误差信号反馈方式，与我们先前相对复杂的方式一样有效。那么有了误差反向传播的相关函数，接下来我们又应该怎样利用它来更新参数呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实验总结"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " 神经网络通过调整链接权重进行学习。这种方法由误差引导，误差就是训练数据所给出正确答案和实际输出之间的差值。然而，与内部节点相关联的误差并不显而易见。一种方法是按照链接权重的比例来分割输出层的误差，然后在每个内部节点处重组这些误差。 接下来的实验中，我们将通过每一层得到的误差，来更新我们的权重，使我们的模型开始学习。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr><div style=\"color: #999; font-size: 12px;\"><i class=\"fa fa-copyright\" aria-hidden=\"true\"> 本课程内容版权归实验楼所有，禁止转载、下载及非法传播。</i></div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
